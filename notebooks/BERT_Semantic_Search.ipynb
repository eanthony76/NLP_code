{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbcf63e4",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aad75d",
   "metadata": {},
   "source": [
    "The purpose of the code in this notebook is to take a Pandas Dataframe containing documents and be able to match a before-unseen block of text to our domain-specific documents found in the dataframe.\n",
    "\n",
    "To perform our matching task, we will use a BERT model which has been previously fine-tuned on Sentence-BERT tasks (outlined in https://arxiv.org/abs/1908.10084 \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\"). This model is designed to take in two document embeddings (less than 512 BERT tokens) and compare them using dot-product or cosine similarity.\n",
    "\n",
    "One weakness of this approach is that off-the-shelf models do not transfer well to unique domains, and so in order to further fine-tune a BERT model for our purposes, we will use the transformer model FLAN T-5 to generate summaries for each of our documents, and then fine-tune the BERT model on a summary-source match task. Inspiration for this approach comes from BEIR's paper outlining its choice to perform synthetic text generation in order to append documents with semantically similar text to improve BM-25 search results (https://arxiv.org/abs/2104.08663 \"BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models\")\n",
    "\n",
    "Once we have fine-tuned our BERT model on our specific domain, we can now embed the documents using the model and compare new blocks of text with our document store. \n",
    "\n",
    "We will be using FAISS to index our documents and perform our matching. For more information on FAISS indices, please visit their github (https://github.com/facebookresearch/faiss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430afcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util, losses, models, datasets, InputExample, CrossEncoder\n",
    "from torch import nn\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel, T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "from accelerate import Accelerator\n",
    "from matplotlib import pyplot as plt\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from cross_enc_search import cross_search, cross_fetch_article_info\n",
    "\n",
    "data = pd.read_csv('swcs_text_data.csv')\n",
    "# data = joblib.load('fulldataset.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f9bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a new dataframe column called 'body'\n",
    "data['body'] = data.text.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37450c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This makes sure all of the body text is in the same format before sending it to be encoded\n",
    "data['body'] = [''.join(map(str, l)) for l in data['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f2899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Helper function which fetches article info for query-article match. Right now it is formatted to pull the body \n",
    "column from the data dataframe, but can be adjusted'''\n",
    "def fetch_article_info(dataframe_idx):\n",
    "    info = data.iloc[dataframe_idx]\n",
    "    meta_dict = dict()\n",
    "#     meta_dict['Title'] = info['Title']\n",
    "    meta_dict['Body'] = info['body']\n",
    "    return meta_dict\n",
    "    \n",
    "'''Helper function which encodes the query using the BERT model and then performs a search to try to match the query vector\n",
    "to the top k most similar articles'''\n",
    "def search(query, top_k, index, model):\n",
    "    t=time.time()\n",
    "    query_vector = model.encode([query])\n",
    "    distances, top_k = index.search(query_vector, top_k)\n",
    "    print('>>>> Time to return results: {}'.format(time.time()-t))\n",
    "    top_k_ids = top_k.tolist()[0]\n",
    "    top_k_ids = list(np.unique(top_k_ids))\n",
    "    results = zip([fetch_article_info(idx) for idx in top_k_ids], distances[0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c415083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eanthony/miniconda3/envs/super-base/lib/python3.10/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Time to return results: 0.02078390121459961\n",
      ">>>> Time to return results: 0.016375303268432617\n",
      "[[3042, 6.948469], [72, 6.943064], [3202, 6.7963085], [3793, 6.7963085], [3766, 6.795002], [3175, 6.7950015], [666, 6.709031], [636, 6.677759], [380, 4.4303737], [3039, 3.8847961], [3555, 3.8847961], [554, 3.0714765], [2724, 3.0152762], [3414, 3.0152762], [2827, 2.0439653]]\n",
      "\n",
      "\n",
      "\t \n",
      " [[6.948469, 3042    Analyst Note:  The WTU is the USSP branch curr...\n",
      "6       Open Source Intelligence (OSINT)/Human Intelli...\n",
      "Name: body, dtype: object], [6.943064, 72    Analyst Note: The WTU is the USSP branch curre...\n",
      "6     Open Source Intelligence (OSINT)/Human Intelli...\n",
      "Name: body, dtype: object], [6.7963085, 3202    The WTU is promoting a dedicated, nonviolent r...\n",
      "6       Open Source Intelligence (OSINT)/Human Intelli...\n",
      "Name: body, dtype: object], [6.7963085, 3793    The WTU is promoting a dedicated, nonviolent r...\n",
      "6       Open Source Intelligence (OSINT)/Human Intelli...\n",
      "Name: body, dtype: object]]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import time\n",
    "index = faiss.read_index('body_paragraphs.index')\n",
    "model = SentenceTransformer('search/search-model-t5-large-queries')\n",
    "\n",
    "query=\"Who is the WTU\"\n",
    "results=search(query, top_k=5, index=index, model=model)\n",
    "\n",
    "cross_results_all = cross_search(query, model=model, data=data)\n",
    "\n",
    "print(\"\\n\")\n",
    "for result in cross_results_all:\n",
    "    print('\\t','\\n',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d3a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'google/flan-t5-large' #Time to complete 3795 paragraphs w/ num_queries = 5, batch_size = 64, max_query_length = 64: 1:45:06\n",
    "model_name = 'google/flan-t5-base' #Time to complete 3795 paragraphs w/ num_queries = 5, batch_size = 64, max_query_length = 64: 58:04\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "def _removeNonAscii(s): \n",
    "    return \"\".join(i for i in s if ord(i) < 128)\n",
    "\n",
    "# Parameters for generation\n",
    "batch_size = 64 #Batch size\n",
    "num_queries = 5 #Number of queries to generate for every paragraph\n",
    "max_length_query = 64   #Max length for output query\n",
    "\n",
    "corpus = data.body\n",
    "k = 0\n",
    "\n",
    "'''Now we create a new tsv which will store our summary-source pairs'''\n",
    "\n",
    "with open('generated_queries_t5-base(2).tsv', 'w') as fOut:\n",
    "    for para in tqdm(corpus):\n",
    "        para = str(corpus.values[k])\n",
    "        pre_para= 'summarize:'+para\n",
    "        index = corpus.index[k]\n",
    "        input_ids = tokenizer.encode(pre_para, truncation=True, return_tensors='pt').to(device)\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length_query,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=num_queries)\n",
    "\n",
    "        for i in range(len(outputs)):\n",
    "            query = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "            query = _removeNonAscii(query)\n",
    "            para = _removeNonAscii(para)\n",
    "            fOut.write(\"{}\\t{}\\n\".format(query,para))\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ca9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create the training dataset using the tsv we just created.'''\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, datasets\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "log = []\n",
    "train_examples = [] \n",
    "with open('generated_queries_t5-base(2).tsv') as fIn:\n",
    "    for line in fIn:\n",
    "        try:\n",
    "            query, paragraph = line.strip().split('\\t', maxsplit=1)\n",
    "            train_examples.append(InputExample(texts=[query, paragraph]))\n",
    "        except:\n",
    "            log.append(\"error\")\n",
    "            print(line)\n",
    "            pass\n",
    "    print(\"The following number of examples could not be appended into your training examples: {} out of {}\".format(len(log), len(train_examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now we fine tune our model using the dataset we created from the tsv'''\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-distilroberta-v1\")\n",
    "train_dataloader = datasets.NoDuplicatesDataLoader(train_examples, batch_size=8)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "accelerator = Accelerator()\n",
    "\n",
    "num_epochs = 3\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], \n",
    "          epochs=num_epochs, \n",
    "          warmup_steps=warmup_steps, \n",
    "          show_progress_bar=True,\n",
    "         accelerator=accelerator)\n",
    "\n",
    "'''save our fine-tuned model to disk'''\n",
    "\n",
    "os.makedirs('search', exist_ok=True)\n",
    "model.save('search/search-model-t5-base-queries')\n",
    "#Time to fine tune \"sentence-transformers/all-distilroberta-v1\": 10 mins 1 sec with 3795 paragraphs w/ 5 queries each (18903)\n",
    "#Time to fine tune \"sentence-transformers/all-distilroberta-v1\": 10 mins 1 sec with 3795 paragraphs w/ 5 queries each (18903)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load model checkpoint and encode documents again using fine-tuned model'''\n",
    "model = SentenceTransformer('search/search-model-t5-large-queries')\n",
    "#7 seconds to encode 3795 paragraphs using fine-tuned distilroberta model\n",
    "encoded_data = model.encode(data.body.tolist(), show_progress_bar=True)\n",
    "encoded_data = np.asarray(encoded_data.astype('float32'))\n",
    "\n",
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\n",
    "ids = np.array(range(0, len(data)), dtype='int64')\n",
    "index.add_with_ids(encoded_data, ids)\n",
    "\n",
    "# faiss.normalize_L2(encoded_data)\n",
    "# index_cosine = faiss.IndexFlat(768, faiss.METRIC_INNER_PRODUCT)\n",
    "# index_cosine.add(encoded_data)\n",
    "\n",
    "\n",
    "faiss.write_index(index, 'body_paragraphs.index')\n",
    "# faiss.write_index(index_cosine, 'body_paragraphs_cosine.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf789507",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29499ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import time\n",
    "index = faiss.read_index('body_paragraphs.index')\n",
    "model = SentenceTransformer('search/search-model-t5-large-queries')\n",
    "\n",
    "query=\"Who is the WTU\"\n",
    "results=search(query, top_k=5, index=index, model=model)\n",
    "\n",
    "print(\"\\n\")\n",
    "for result in results:\n",
    "    print('\\t','\\n',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "query = \"President Canteth recently made the controversial move to make Mr. David Patton the new Governor of the Northern Pineland Province.\"\n",
    "results=search(query, top_k=5, index=index, model=model)\n",
    "\n",
    "print(\"\\n\")\n",
    "for result in results:\n",
    "    print('\\t','\\n',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5739ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
